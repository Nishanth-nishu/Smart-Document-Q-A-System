# DocKnowledge — Smart Document Q&A System

> **AI-powered document Q&A with user authentication, per-user data isolation, and source citations.**

Built for the AI Engineer Take-Home Assignment — Option 1: Smart Document Q&A System.

---

**Live Demo**: [http://16.176.103.66:3000/](http://16.176.103.66:3000/)
**Test credentials**: `test@docknowledge.app` / `Demo1234!`

---

## Table of Contents

- [What It Does](#what-it-does)
- [Architecture](#architecture)
- [RAG Pipeline](#rag-pipeline)
- [Quick Start (Local)](#quick-start-local)
- [Deployment](#deployment)
- [Project Structure](#project-structure)
- [API Reference](#api-reference)
- [Environment Variables](#environment-variables)
- [Security](#security)

---

## What It Does

DocKnowledge lets users upload PDF and text files, then ask natural language questions about them. Answers are generated by an LLM and grounded in the actual content of the uploaded documents, with source citations showing exactly which file and page each piece of information came from.

Key capabilities:

- Email/password authentication with per-user data isolation
- Upload PDF and TXT documents (up to 20 MB each)
- Asynchronous document processing with real-time status polling
- Hybrid semantic + keyword search using sentence-window RAG
- Answers with inline `[Source N]` citations and expandable source cards
- Document management: list, status tracking, delete

---

## Architecture

```
Browser (Next.js 14 — Vercel / EC2)
          │
          │  HTTPS + JWT Bearer token
          ▼
FastAPI Backend (Python — Render / EC2)
          │
    ┌─────┼──────────────┐
    ▼     ▼              ▼
Supabase          Supabase         Google Gemini
PostgreSQL        pgvector         (default LLM)
(users,           (384-dim         /
documents)        embeddings)      OpenRouter
                                   (fallback LLM)
          │
  sentence-transformers
  all-MiniLM-L6-v2
  (local, no API cost)
```

| Layer | Technology | Why |
|---|---|---|
| Frontend | Next.js 14 (App Router, TypeScript) | Industry standard React framework, Vercel-native, full control over auth/UX |
| Backend | FastAPI (Python) | Async, fast, great ML ecosystem, built-in OpenAPI docs |
| Database | Supabase (PostgreSQL) | Built-in pgvector, single platform for all data, no extra vector DB needed |
| Embeddings | sentence-transformers/all-MiniLM-L6-v2 | Free, local, 384-dim, fits in 512 MB RAM on Render free tier |
| LLM (default) | Google Gemini (`gemini-2.0-flash`) | Free tier, reliable, no rate limits at prototype scale |
| LLM (fallback) | OpenRouter / Llama 3.1 | Switchable via env var, also free |
| Deployment | EC2 (backend + frontend) / Render + Vercel | Zero DevOps overhead, GitHub integration |

---

## RAG Pipeline

Based on published research:

- **Sentence Window Retrieval** — Jerry Liu, LlamaIndex (2023): embed small anchor sentences, retrieve larger context windows for the LLM
- **Reciprocal Rank Fusion** — Cormack et al., SIGIR 2009: merge ranked lists from multiple retrieval methods
- **Dense Passage Retrieval** — Karpukhin et al., ACL 2020: dense retrieval outperforms pure BM25 on semantic queries; hybrid beats both

```
─── Document Ingestion ──────────────────────────────────────
  File bytes
      │
      ▼
  Split into sentences (NLTK sent_tokenize)
      │
      ▼
  Build sentence windows (anchor ± 2 neighbors)
  → sentence_text  : anchor sentence   → embedded for search
  → window_text    : anchor + context  → fed to LLM
      │
      ▼
  Embed anchor sentences (all-MiniLM-L6-v2, 384-dim)
      │
      ▼
  Store chunks + embeddings in Supabase pgvector

─── Q&A Query ───────────────────────────────────────────────
  User question
      │
      ├─► Dense path  : embed query → pgvector cosine search → top-5
      │
      └─► Sparse path : BM25Okapi on all user chunks → top-5
      │
      ▼
  Reciprocal Rank Fusion (k=60) → top-4 fused chunks
      │
      ▼
  Build prompt: window_text of top-4 chunks as [Source N] context
      │
      ▼
  LLM (Gemini / OpenRouter) → answer with inline citations
      │
      ▼
  Return: answer + source citations (filename, page, relevance score)
```

---

## Quick Start (Local)

### Prerequisites

- Python 3.11+
- Node.js 20+
- A [Supabase](https://supabase.com) project (free tier works)
- A [Google Gemini](https://aistudio.google.com/app/apikey) API key (free) **or** an [OpenRouter](https://openrouter.ai) API key (free)

### 1. Clone and configure

```bash
git clone <your-repo-url>
cd rag-chatbot-python-fullstack-template

cp .env.example .env
# Edit .env — fill in SUPABASE_*, GEMINI_API_KEY (or OPENROUTER_API_KEY), JWT_SECRET
```

### 2. Set up the database

1. Go to [app.supabase.com](https://app.supabase.com) → your project → **SQL Editor**
2. Paste the full contents of `supabase_schema.sql` and run it
3. This creates: `users`, `documents`, `document_chunks` tables + pgvector IVFFlat index + `match_document_chunks` RPC function
4. Optionally, create a **private** Storage bucket named `documents` via the Supabase Dashboard

### 3. Run the backend

```bash
cd backend
pip install -r ../requirements/backend_requirements.txt
uvicorn api:app --reload --port 8000
```

API available at: `http://localhost:8000`
Interactive docs: `http://localhost:8000/docs`

### 4. Run the frontend

```bash
cd frontend-app
cp .env.local.example .env.local
# Ensure NEXT_PUBLIC_API_URL=http://localhost:8000

npm install
npm run dev
```

Frontend at: `http://localhost:3000`

### 5. Run with Docker Compose (alternative)

```bash
# Ensure .env is filled in
docker-compose up --build
```

This starts both backend (port 8000) and frontend (port 3000) together.

---

## Deployment

### Backend → Render (Free tier)

1. Push code to GitHub
2. Go to [render.com](https://render.com) → **New Web Service** → connect your repo
3. Settings:
   - **Runtime**: Python 3
   - **Build Command**: `pip install -r requirements/backend_requirements.txt`
   - **Start Command**: `uvicorn backend.api:app --host 0.0.0.0 --port $PORT`
4. Add all variables from `.env.example` under **Environment**
5. Deploy → copy the URL (e.g. `https://docknowledge-api.onrender.com`)

### Frontend → Vercel (Free tier)

1. Go to [vercel.com](https://vercel.com) → **New Project** → import your repo
2. Settings:
   - **Framework**: Next.js
   - **Root Directory**: `frontend-app`
3. Add environment variable: `NEXT_PUBLIC_API_URL` = your Render backend URL
4. Deploy

### Update CORS

After deploying, set `FRONTEND_URL` in your Render environment variables to your Vercel URL so the backend's CORS allowlist includes your frontend.

### Deployment via Docker Hub

```powershell
# Builds and pushes both images to Docker Hub (nishanthr23)
./push_images.ps1
```

Images:
- `nishanthr23/docknowledge-backend:latest`
- `nishanthr23/docknowledge-frontend:latest`

---

## Project Structure

```
rag-chatbot-python-fullstack-template/
│
├── backend/
│   ├── api.py              # FastAPI app — all REST endpoints
│   ├── rag_engine.py       # RAG pipeline (ingestion, hybrid search, LLM calls)
│   ├── database.py         # Supabase data layer (users, documents, chunks, vector search)
│   ├── auth.py             # JWT creation/validation, bcrypt password hashing
│   ├── config.py           # Settings loaded from environment variables
│   └── schemas.py          # Pydantic request/response models
│
├── frontend-app/
│   ├── app/
│   │   ├── page.tsx            # Landing page
│   │   ├── login/page.tsx      # Login form
│   │   ├── register/page.tsx   # Registration form
│   │   ├── dashboard/page.tsx  # Document upload & management
│   │   └── chat/page.tsx       # Q&A chat interface with source citations
│   ├── lib/
│   │   └── api.ts              # Typed API client (all fetch calls)
│   ├── Dockerfile
│   └── package.json
│
├── docker/
│   └── backend.Dockerfile
│
├── requirements/
│   └── backend_requirements.txt
│
├── supabase_schema.sql     # Full DB schema — run once in Supabase SQL Editor
├── docker-compose.yml      # Local full-stack dev environment
├── push_images.ps1         # Docker Hub push script
├── .env.example            # Environment variable template
├── DOCUMENTATION.md        # Architecture decisions, trade-offs, AI tool usage
└── README.md               # This file
```

---

## API Reference

All protected endpoints require `Authorization: Bearer <token>` header.

| Method | Endpoint | Auth | Description |
|---|---|---|---|
| `POST` | `/auth/register` | — | Register with email + password → returns JWT |
| `POST` | `/auth/login` | — | Login → returns JWT |
| `GET` | `/auth/me` | ✅ | Current user info |
| `POST` | `/documents/upload` | ✅ | Upload PDF or TXT → 202 Accepted (async processing) |
| `GET` | `/documents/` | ✅ | List all user's documents |
| `GET` | `/documents/{id}` | ✅ | Single document details + status |
| `DELETE` | `/documents/{id}` | ✅ | Delete document and all its chunks |
| `POST` | `/ask` | ✅ | Ask a question (optionally filter by `document_ids`) |
| `GET` | `/health` | — | Health check |

### Request/Response examples

**Upload a document**
```http
POST /documents/upload
Content-Type: multipart/form-data
Authorization: Bearer <token>

file: <binary>
```
```json
{
  "document_id": "uuid",
  "filename": "report.pdf",
  "status": "processing",
  "message": "Document uploaded. Processing will complete in a few seconds."
}
```

**Ask a question**
```http
POST /ask
Content-Type: application/json
Authorization: Bearer <token>

{
  "question": "What are the main findings?",
  "document_ids": ["uuid1", "uuid2"]   // omit to search all documents
}
```
```json
{
  "answer": "The main findings are... [Source 1] ... [Source 2]",
  "sources": [
    {
      "document_id": "uuid1",
      "filename": "report.pdf",
      "page_number": 3,
      "chunk_text": "...",
      "relevance_score": 0.0412
    }
  ],
  "status": "success"
}
```

---

## Environment Variables

| Variable | Required | Description |
|---|---|---|
| `SUPABASE_URL` | ✅ | Supabase project URL |
| `SUPABASE_ANON_KEY` | ✅ | Supabase anon/public key |
| `SUPABASE_SERVICE_ROLE_KEY` | ✅ | Supabase service role key (server-side ops) |
| `LLM_PROVIDER` | ✅ | `gemini` (default) or `openrouter` |
| `GEMINI_API_KEY` | ✅ | Google Gemini API key ([get free key](https://aistudio.google.com/app/apikey)) |
| `GEMINI_MODEL` | — | Default: `gemini-2.0-flash` |
| `OPENROUTER_API_KEY` | — | OpenRouter API key (if using as fallback) |
| `OPENROUTER_MODEL` | — | Default: `meta-llama/llama-3.1-8b-instruct:free` |
| `JWT_SECRET` | ✅ | Random secret string for JWT signing (min 32 chars) |
| `JWT_EXPIRE_MINUTES` | — | Default: `1440` (24 hours) |
| `FRONTEND_URL` | ✅ | Deployed frontend URL (for CORS allowlist) |
| `NEXT_PUBLIC_API_URL` | ✅ | Backend URL (used by Next.js client) |

---

## Security

- **Passwords** — bcrypt-hashed with random salt; never stored in plaintext
- **Auth tokens** — JWT (HS256), 24-hour expiry, validated on every protected request
- **Data isolation** — every DB query is scoped by `user_id`; users cannot access each other's documents or chunks
- **CORS** — allowlist restricted to the configured `FRONTEND_URL`
- **File validation** — only PDF and TXT accepted; 20 MB maximum; empty files rejected
- **Known prototype limitations** — JWT stored in localStorage (XSS risk in production); no HTTPS enforced on the demo server; no refresh token flow

---

## Running Tests

There are no automated tests in this prototype (omitted given the 4–6 hour constraint). The `/health` endpoint can be used as a basic liveness check:

```bash
curl http://localhost:8000/health
# {"status":"ok","service":"DocKnowledge Q&A"}
```
